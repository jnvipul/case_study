{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"data\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "%matplotlib inline\n",
    "import statistics\n",
    "# from fuzzywuzzy import fuzz\n",
    "import nltk.tokenize as nt\n",
    "import nltk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from joblib import dump, load\n",
    "import json\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    # standard deviation threshold\n",
    "    sd_threshold = 1\n",
    "    \n",
    "    # Remove price outliers\n",
    "    df = df[(df.price <= 15000) & (df.price >= 1000)]\n",
    "    \n",
    "    # Remove dist from city centre outliers\n",
    "    # apporimate radius from city centre\n",
    "    NYC_RADIUS = 20\n",
    "    df = df[(df.dist_from_ctr <= 20)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "manager_scores = {}\n",
    "def create_manager_scores(df):\n",
    "    global manager_scores\n",
    "    manager_scores = {}\n",
    "    \n",
    "    def calculate_manager_score(row):\n",
    "        manager_id = row['manager_id']\n",
    "        interest = row['interest_level']\n",
    "        \n",
    "        score_to_add = 0\n",
    "        if interest == 'high':\n",
    "            score_to_add += 3\n",
    "        elif interest == 'medium':\n",
    "            score_to_add += 2\n",
    "        elif interest == 'low':\n",
    "            score_to_add += 1\n",
    "        \n",
    "        if manager_id in manager_scores:\n",
    "            manager_scores[manager_id].append(score_to_add)\n",
    "        else:\n",
    "            manager_scores[manager_id] = [score_to_add]\n",
    "    df.apply(calculate_manager_score, axis=1)\n",
    "\n",
    "def apply_manager_scores(row):\n",
    "    manager_id = row['manager_id']\n",
    "    \n",
    "    if manager_id in manager_scores:\n",
    "        row['manager_score'] = sum(manager_scores[manager_id])/len(manager_scores[manager_id])\n",
    "    else:\n",
    "        row['manager_score'] = 0\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "building_scores = {}\n",
    "def create_building_scores(df):\n",
    "    global building_scores\n",
    "    building_scores = {}\n",
    "    \n",
    "    def calculate_building_score(row):\n",
    "        building_id = row['building_id']\n",
    "        interest = row['interest_level']\n",
    "        \n",
    "        score_to_add = 0\n",
    "        if interest == 'high':\n",
    "            score_to_add += 3\n",
    "        elif interest == 'medium':\n",
    "            score_to_add += 2\n",
    "        elif interest == 'low':\n",
    "            score_to_add += 1\n",
    "        \n",
    "        if building_id in building_scores:\n",
    "            building_scores[building_id].append(score_to_add)\n",
    "        else:\n",
    "            building_scores[building_id] = [score_to_add]\n",
    "        \n",
    "    df.apply(calculate_building_score, axis=1)\n",
    "    \n",
    "    # NOTE : Building ID '0' seem to be missing data issue | Assigning 0 score to building id 0   \n",
    "    building_scores['0'] = [0]\n",
    "    \n",
    "def apply_building_scores(row):\n",
    "    building_id = row['building_id']\n",
    "    if building_id in building_scores:\n",
    "        row['building_score'] = sum(building_scores[building_id])/len(building_scores[building_id])\n",
    "    else:\n",
    "        row['building_score'] = 0\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def price_per_bedroom(row):\n",
    "    bedrooms = row['bedrooms']\n",
    "    if bedrooms == 0:\n",
    "        price_per_bedroom = 0\n",
    "    else:\n",
    "        price_per_bedroom = row['price'] * 1.00 / bedrooms\n",
    "    row['price_per_bedroom'] = price_per_bedroom\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def price_per_bathroom(row):\n",
    "    bathrooms = row['bathrooms']\n",
    "    if bathrooms == 0:\n",
    "        price_per_bathroom = 0\n",
    "    else:\n",
    "        price_per_bathroom = row['price'] * 1.00 / bathrooms\n",
    "    row['price_per_bathroom'] = price_per_bathroom\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def price_per_total_room(row):\n",
    "    rooms = row['total_rooms']\n",
    "    if rooms == 0:\n",
    "        price_per_total_rooms = 0\n",
    "    else:\n",
    "        price_per_total_rooms = row['price'] * 1.00 / rooms\n",
    "    row['price_per_total_rooms'] = price_per_total_rooms\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bath_bed_ratio(row):\n",
    "    bedrooms = row['bedrooms']\n",
    "    bathrooms = row['bathrooms']\n",
    "    if bedrooms == 0:\n",
    "        bath_bed_ratio = bathrooms\n",
    "    else:\n",
    "        bath_bed_ratio = bathrooms/bedrooms\n",
    "    \n",
    "    row['bath_bed_ratio'] = bath_bed_ratio\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_missing_values(test_df):\n",
    "    missing_cols = set(get_feature_list()) - set(test_df.columns)\n",
    "    # Add a missing column in test set with default value equal to 0\n",
    "    for c in missing_cols:\n",
    "        test_df[c] = 0\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Key : Feature in data | Value : column name to be created for category variable\n",
    "\n",
    "home_features_dict = {'Elevator': 'has_elevator',\n",
    " 'Cats Allowed': 'has_cats_allowed',\n",
    " 'Hardwood Floors': 'has_hardwood_floors',\n",
    " 'Dogs Allowed': 'has_dogs_allowed',\n",
    " 'Doorman': 'has_doorman',\n",
    " 'Dishwasher': 'has_dishwasher',\n",
    " 'No Fee': 'has_no_fee',\n",
    " 'Laundry in Building': 'has_laundry_in_building',\n",
    " 'Fitness Center': 'has_fitness_center',\n",
    " 'Pre-War': 'has_pre-war',\n",
    " 'Laundry in Unit': 'has_laundry_in_unit',\n",
    " 'Roof Deck': 'has_roof_deck',\n",
    " 'Outdoor Space': 'has_outdoor_space',\n",
    " 'Dining Room': 'has_dining_room',\n",
    " 'High Speed Internet': 'has_high_speed_internet',\n",
    " 'Balcony': 'has_balcony',\n",
    " 'Swimming Pool': 'has_swimming_pool',\n",
    " 'Laundry In Building': 'has_laundry_in_building',\n",
    " 'New Construction': 'has_new_construction',\n",
    " 'Terrace': 'has_terrace',\n",
    " 'Exclusive': 'has_exclusive',\n",
    " 'Loft': 'has_loft',\n",
    " 'Garden/Patio': 'has_garden/patio',\n",
    " 'Wheelchair Access': 'has_wheelchair_access',\n",
    " 'Common Outdoor Space': 'has_common_outdoor_space'}\n",
    "\n",
    "def process_home_features(df):\n",
    "    # Add columns for popular features\n",
    "    for key, val in home_features_dict.items():\n",
    "        df[val] = 0\n",
    "        \n",
    "    def update_popular_feature_cols(row):\n",
    "        features = row['features']\n",
    "        for feature in features:\n",
    "            if feature in home_features_dict:\n",
    "                row[home_features_dict[feature]] = 1\n",
    "\n",
    "        return row\n",
    "    \n",
    "    df = df.apply(update_popular_feature_cols, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_has_photos_has_description(row):\n",
    "    row['has_photos'] = 1 if row['num_photos'] > 0 else 0\n",
    "    row['has_description'] = 1 if row['num_description_words'] > 0 else 0\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "def distance_from_centre(row):\n",
    "    centre = (40.718, -74.008)\n",
    "    lat_long = (row['latitude'], row['longitude'])\n",
    "    distance = geopy.distance.geodesic(centre, lat_long).miles\n",
    "    row['dist_from_ctr'] = distance\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def street_and_display_address_similarity(row):\n",
    "    street_ad = row['street_address'].lower()\n",
    "    display_ad = row['display_address'].lower()\n",
    "    row['address_similarity'] = 0 if (fuzz.ratio(street_ad, display_ad)/100) <= 0.5 else 1\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_adjectives(text):\n",
    "    if not text:\n",
    "        return 0\n",
    "    ss=nt.sent_tokenize(text)\n",
    "    tokenized_sent=[nt.word_tokenize(sent) for sent in ss]\n",
    "    pos_sentences=[nltk.pos_tag(sent) for sent in tokenized_sent]\n",
    "    pos_sentences\n",
    "    adjectives = 0\n",
    "    for pos in pos_sentences:\n",
    "        for pair in pos:\n",
    "            tag = pair[1]\n",
    "            if tag in ['JJ', 'JJR', 'JJS']:\n",
    "                adjectives += 1\n",
    "\n",
    "    return adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_adjectives_column(row):\n",
    "    description = row[\"description\"]\n",
    "    row['num_adjectives_description'] = get_num_adjectives(description)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bucket_hour(hour):\n",
    "    if hour > 4 and hour <= 10:\n",
    "        return 'morning'\n",
    "    elif hour > 10 and hour <= 16:\n",
    "        return 'noon'\n",
    "    elif hour > 16 and hour <= 22:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_dummies = set()\n",
    "def one_hot_encode(df, cols, is_test_set):\n",
    "    # Get one hot encoding of column\n",
    "    df_processed = pd.get_dummies(df, prefix_sep=\"__\",\n",
    "                              columns=cols)\n",
    "    # save all categorical variables\n",
    "    global cat_dummies\n",
    "    if not is_test_set:\n",
    "        cat_dummies = set([col for col in df_processed \n",
    "                   if \"__\" in col \n",
    "                   and col.split(\"__\")[0] in cols])\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df, is_test_set=False):\n",
    "    df[\"num_photos\"] = df[\"photos\"].apply(len)\n",
    "    df[\"num_features\"] = df[\"features\"].apply(len)\n",
    "    df[\"num_description_words\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "    # Create date month year\n",
    "    df[\"created\"] = pd.to_datetime(df[\"created\"])\n",
    "    df[\"created_month\"] = df[\"created\"].dt.month\n",
    "    df[\"created_day\"] = df[\"created\"].dt.day\n",
    "    df[\"created_hour\"] = df[\"created\"].dt.hour\n",
    "    df[\"created_day_of_week\"] = df[\"created\"].dt.dayofweek\n",
    "    df[\"created_day_of_month\"] = df[\"created\"].dt.day\n",
    "    df['is_weekday'] = ((df.created_day_of_week) // 5 == 1).astype(float)\n",
    "    df = df.apply(apply_manager_scores, axis=1)\n",
    "    df = df.apply(price_per_bedroom, axis=1)\n",
    "    df = process_home_features(df)\n",
    "    df = df.apply(distance_from_centre, axis=1)\n",
    "    \n",
    "    # if length is more than 2 - has address\n",
    "    df['has_display_address'] = df['display_address'].apply(lambda x : 1 if len(x) > 2 else 0)\n",
    "    df['has_street_address'] = df['street_address'].apply(lambda x : 1 if len(x) > 2 else 0)\n",
    "    df['total_rooms'] = df['bedrooms'] + df['bathrooms']\n",
    "    df = df.apply(price_per_total_room, axis=1)\n",
    "#     df = df.apply(apply_building_scores, axis=1)\n",
    "    \n",
    "    # One hot encodings\n",
    "    df = one_hot_encode(df, ['created_hour', 'created_month'], is_test_set)\n",
    "    \n",
    "    if is_test_set:\n",
    "        add_missing_values(df)\n",
    "    \n",
    "    # Didn't work\n",
    "    # df = df.apply(create_has_photos_has_description, axis=1)\n",
    "    # df = df.apply(price_per_bathroom, axis=1) \n",
    "    # df = df.apply(bath_bed_ratio, axis=1)\n",
    "    # df[\"bed_bath_differnce\"] = df['bedrooms'] - df['bathrooms']\n",
    "    # df[\"bed_bath_sum\"] = df[\"bedrooms\"] + df['bathrooms']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_list():\n",
    "    num_feats = [\"bathrooms\", \n",
    "             \"bedrooms\", \n",
    "             \"latitude\", \n",
    "             \"longitude\", \n",
    "             \"price\",\n",
    "             \"num_photos\", \n",
    "             \"num_features\", \n",
    "             \"num_description_words\",\n",
    "             \"created_day_of_month\",\n",
    "             \"is_weekday\",\n",
    "             \"manager_score\",\n",
    "             \"dist_from_ctr\",\n",
    "             \"has_display_address\", \n",
    "             \"has_street_address\",\n",
    "             \"total_rooms\",\n",
    "             \"price_per_total_rooms\"\n",
    "#              \"num_adjectives_description\"\n",
    "            ]\n",
    "    \n",
    "    # Add one hot encoded variables\n",
    "    global cat_dummies\n",
    "    num_feats.extend(cat_dummies)\n",
    "    \n",
    "    # add names of house features\n",
    "    num_feats.extend(set(home_features_dict.values()))\n",
    "    return num_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(open(\"data/train.json\", \"r\"))\n",
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_manager_scores(df)\n",
    "# create_building_scores(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "print (str(currentDT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = feature_engineering(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = remove_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDT = datetime.datetime.now()\n",
    "print (str(currentDT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df[get_feature_list()]\n",
    "y = df[\"interest_level\"]\n",
    "X.head()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "clf = get_optimised_clf_grid_search(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "y_val_pred = clf.predict_proba(X_val)\n",
    "\n",
    "# calculate training loss\n",
    "loss = log_loss(y_val, y_val_pred)\n",
    "print(f'Loss : {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random SearchCV\n",
    "Idea is to narrow down our search for hyperparameters after evaluate a wide range of values for each hyperparameter randomly.\n",
    "(Later we can use GridSearchCV to choose the best out of the narrowed ones as we will have a better idea of the ballparks)\n",
    "\n",
    "\n",
    "*NOTE* : Result from RandomSearch CV - {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 60, 'bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5] + [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n",
    "\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "\n",
    "Result Params after running grid_search - {'bootstrap': False, 'max_depth': 40, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimised_clf_grid_search(X_train, y_train):\n",
    "    # NOTE :  the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'bootstrap': [False],\n",
    "        'max_depth': [40, 50, 60, 70, 80],\n",
    "        'max_features': ['sqrt'],\n",
    "        'min_samples_leaf': [1, 2, 3, 4],\n",
    "        'min_samples_split': [8, 10, 12],\n",
    "        'n_estimators': [100, 200, 300, 600]\n",
    "    }\n",
    "    # Create a based model\n",
    "    rf = RandomForestClassifier()\n",
    "    # Instantiate the grid search model\n",
    "    clf = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                              cv = 5, n_jobs = -1, verbose = 2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.best_params_)\n",
    "    optimised_clf = clf.best_estimator_\n",
    "    return optimised_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump(clf, 'model.joblib')\n",
    "dump(scaler, \"scaler.save\")\n",
    "with open('manager_scores.json', 'w') as fp:\n",
    "    json.dump(manager_scores, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(index = get_feature_list(), data = clf.feature_importances_).sort_values().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and feature engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_json(open(\"data/test.json\", \"r\"))\n",
    "test_df = feature_engineering(test_df, is_test_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "test_df[get_feature_list()].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test = test_df[get_feature_list()]\n",
    "X_test = scaler.transform(X_test)\n",
    "y = clf.predict_proba(X_test)\n",
    "\n",
    "labels2idx = {label: i for i, label in enumerate(clf.classes_)}\n",
    "sub = pd.DataFrame()\n",
    "sub[\"listing_id\"] = test_df[\"listing_id\"]\n",
    "for label in [\"high\", \"medium\", \"low\"]:\n",
    "    sub[label] = y[:, labels2idx[label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(index = get_feature_list(), data = clf.feature_importances_).sort_values().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=1000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_df[get_feature_list()]\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "y = np.array(df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(X, y, X_test, num_rounds=400)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"xgb_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
